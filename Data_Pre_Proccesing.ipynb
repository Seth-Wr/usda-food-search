{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f923c0c1",
   "metadata": {},
   "source": [
    "Csv \"food.csv\" \"food_nutrient.csv\" \"nutrient.csv\" \"food_category\" are from USDA SR Legacy Foods Dataset. \n",
    "All Csv created i moved to Csv_Data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef678740",
   "metadata": {},
   "source": [
    "Step #1 Load food.csv and take all the words from all description (food names) and create new csv ordered by frequency of word spoken. Make all words lowercase for smoother proccesing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "482fadf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ffaa6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_foods_df = pd.read_csv('Csv_Data/food.csv')\n",
    "raw_foods_df = raw_foods_df.drop_duplicates(subset='description')\n",
    "raw_foods_df = raw_foods_df.dropna(subset='description')\n",
    "raw_foods_df['description'] = raw_foods_df['description'].str.lower()\n",
    "# taking out babyfood category\n",
    "raw_foods_df = raw_foods_df[~raw_foods_df['food_category_id'].isin([3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26e351ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = \" \".join(raw_foods_df['description'].astype(str)).lower()\n",
    "any_word_regex = r'\\b[a-z,0-9]{2,}\\b'\n",
    "raw_foods_words = re.findall(any_word_regex,all_words)\n",
    "count = Counter(raw_foods_words)\n",
    "keywords = [[word, count] for word ,count in count.items()]\n",
    "all_words_df = pd.DataFrame(keywords,columns=['words','count']).sort_values(by='count',ascending=False)\n",
    "all_words_df.to_csv('all_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201ecfb",
   "metadata": {},
   "source": [
    "Step #2 Open up all_words.csv and go over the first 500 most spoken words by hand and decide which words we want to add to our filter to filter out. I created a new csv and typed the words i wanted to filter inside junk_words.csv. After that i made a category column and put the words i wanted to filter into two different categories junk and filler. Junk is words i want to remove from the description columns completely think words like imported, its not really useful to get the macros for the food and is just extra messy string data for UI, the other group is filler which is for words like and I dont want to remove the word and completely but i also dont want to add it into our prefix tree later on so i put the filler words in a category as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f86538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Csv with junk words in category filler or junk\n",
    "junk_words_df = pd.read_csv('Csv_Data/junk_words.csv')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1550cf5",
   "metadata": {},
   "source": [
    "Step #3 Clean up food descriptions with the words from our filter and clean up any columns or hyphens that are left afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e69d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "junk_words = junk_words_df[junk_words_df['category'] == 'junk']\n",
    "filler_words = junk_words_df[junk_words_df['category'] == 'filler']\n",
    "junk_words_regex = r'\\b(?:' + \"|\".join(junk_words['words']) + r')\\b'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af06d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove junk words\n",
    "raw_foods_df['description'] = raw_foods_df['description'].str.replace(junk_words_regex,\"\",regex=True)\n",
    "\n",
    "# Remove excess commas and hyphens\n",
    "raw_foods_df['description'] = raw_foods_df['description'].str.replace(r'([,-]\\s*){2,}', ', ', regex=True)\n",
    "\n",
    "# Replace excess whitespace with one space\n",
    "raw_foods_df['description'] = raw_foods_df['description'].str.replace(r'\\s{2,}', ' ', regex=True)\n",
    "\n",
    "# Removes any combination of commas, hyphens, or spaces  trailing at the end or front \n",
    "raw_foods_df['description'] = raw_foods_df['description'].str.lstrip(',- ')\n",
    "raw_foods_df['description'] = raw_foods_df['description'].str.rstrip(',- ')\n",
    "\n",
    "# Removes parenthesis and its contents inside if the first character inside parenthesis is a space\n",
    "raw_foods_df['description'] = raw_foods_df['description'].str.replace(r'\\s\\(\\s+.*?\\)', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73adad4",
   "metadata": {},
   "source": [
    "Step #4 Add nutrient data to raw foods csv and create updated table replacing old one. Inside nutrient.csv it will show nutrient id for nutrient name and the id associated with the nutrient in food_nutrient.csv we will be connecting each food nutrient value with the fdc_id then putting the measurement inside its own column labeled as the appropriate nutrient. Each nutrient is per 100g of said food and each nutrient is in column amount inside food_nutrient.csv so we will add one by one each nutrient changing the name of column amount to the nutrients name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "baefc911",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_id = 1003\n",
    "fat_id = 1004\n",
    "carbs_id = 1005\n",
    "calories_id = 1008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2706d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_nutrient_df = pd.read_csv('Csv_Data/food_nutrient.csv')\n",
    "# Make filter for each nutrient\n",
    "protein_mask = food_nutrient_df[food_nutrient_df['nutrient_id'] == protein_id]\n",
    "protein_df = protein_mask[['fdc_id','amount']]\n",
    "\n",
    "fat_mask = food_nutrient_df[food_nutrient_df['nutrient_id'] == fat_id]\n",
    "fat_df = fat_mask[['fdc_id','amount']]\n",
    "\n",
    "carbs_mask = food_nutrient_df[food_nutrient_df['nutrient_id'] == carbs_id]\n",
    "carbs_df = carbs_mask[['fdc_id','amount']]\n",
    "\n",
    "calories_mask = food_nutrient_df[food_nutrient_df['nutrient_id'] == calories_id]\n",
    "calories_df = calories_mask[['fdc_id','amount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bd165",
   "metadata": {},
   "source": [
    "Merge each nutrient as a data frame with a left join to not lose any data in raw foods table, save to new csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "246fd3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_protein = raw_foods_df.merge(protein_df,on='fdc_id',how='left')\n",
    "result_protein.rename(columns={'amount': 'protein'},inplace= True)\n",
    "\n",
    "result_fat = result_protein.merge(fat_df,on='fdc_id',how='left')\n",
    "result_fat.rename(columns={'amount': 'fat'},inplace= True)\n",
    "\n",
    "result_carbs = result_fat.merge(carbs_df,on='fdc_id',how='left')\n",
    "result_carbs.rename(columns={'amount': 'carbs'},inplace= True)\n",
    "\n",
    "result_calories = result_carbs.merge(calories_df,on='fdc_id',how='left')\n",
    "result_calories.rename(columns={'amount': 'calories'},inplace= True)\n",
    "\n",
    "# Fill missing columns with 0 and missing calorie columns with Atwater calculation from the other nutrients\n",
    " \n",
    "for col in ['protein', 'fat', 'carbs']:\n",
    "    result_calories[col] = result_calories[col].fillna(0)\n",
    "\n",
    "result_calories['calories'] = result_calories['calories'].fillna(\n",
    "    result_calories['protein']*4 + result_calories['fat']*9 + result_calories['carbs']*4\n",
    ")\n",
    "# Keep only neccasary columns data type and publication date are not needed\n",
    "final_results = result_calories[['fdc_id','description','food_category_id','protein','fat','carbs','calories']]\n",
    "final_results.to_csv('food_macros.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535781c",
   "metadata": {},
   "source": [
    "Step #5 Create food word dictionary of all revelvant search words so we can create a inverted index for our search engine and connect words to documents. For each word we will create a row with the word and fdc_id and categry weight to measure word importance. Create new csv at the end to use for our Search engine file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5131be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove junk and filler words from all_words.csv and create new csv keyword.csv that contains only relevant words\n",
    "\n",
    "keywords_df = all_words_df[~all_words_df['words'].isin(junk_words_df['words'])]\n",
    "keywords_df['words'].to_csv('keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67432d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open food_category.csv and look for what groups you want to prioritize and give a weight value to the group there is 28 categories\n",
    "high_priority = [1,2,4,5,9,10,11,12,13,15,16,17]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d5e24c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both csv's we created earlier the new table and the table with all the words in description\n",
    "food_macros_df = pd.read_csv('Csv_Data/food_macros.csv')\n",
    "keywords_df = pd.read_csv('Csv_Data/keywords.csv') \n",
    "\n",
    "\n",
    "food_dictionary_df = pd.DataFrame(columns={'words': None,'fdc_id': None,'weight': None})\n",
    "# Add | the or operator to each word inside your join statement\n",
    "regex_dictionary_words = r'\\b(?:' + \"|\".join(keywords_df['words'].astype(str)).lower() + r')\\b'\n",
    "\n",
    "# For each row in the food macros we will parse each word that is apart of our recognized word list\n",
    "# For each word in list we will add new entry to dictionary with word and the rows id\n",
    "# Use itertuples to access each rows properties easily\n",
    "for row in food_macros_df.itertuples():\n",
    "    weight = .5\n",
    "    if row.food_category_id in high_priority:\n",
    "        weight = 1.5 \n",
    "    words = row.description\n",
    "    try:\n",
    "        parsed_words = re.findall(regex_dictionary_words,words)\n",
    "        if len(parsed_words) > 0:\n",
    "            for word in parsed_words:\n",
    "                food_dictionary_df.loc[len(food_dictionary_df)] = [word, row.fdc_id, weight]\n",
    "        else:\n",
    "            print(row.description)\n",
    "\n",
    "    except Exception as e:\n",
    "        # I had one row that didnt pass the word screening and is just a \"\" blank white space fdc id 170471 i just deleted manually\n",
    "        print(row.fdc_id)\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "food_dictionary_df.to_csv('food_dictionary.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4975b263",
   "metadata": {},
   "source": [
    "Step #6 Convert csv files into sqlite database to access some powerful query features and better portability to other devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75e0ae68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7447"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_df = pd.read_csv('Csv_Data/keywords.csv')\n",
    "food_dict_df = pd.read_csv('Csv_Data/food_dictionary.csv')\n",
    "food_macros_df = pd.read_csv('Csv_Data/food_macros.csv')\n",
    "conn = sqlite3.connect('search_engine.db')\n",
    "keywords_df.to_sql('keywords_table',conn,index=False,if_exists='replace')\n",
    "food_dict_df.to_sql('food_dict_table',conn,index=False,if_exists='replace')\n",
    "food_macros_df.to_sql('food_macros_table',conn,index=False,if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da7c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Food dictionary on words for faster lookup times\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "CREATE INDEX IF NOT EXISTS idx_food_dict_words\n",
    "ON food_dict_table(words)     \n",
    "               \"\"\")\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c98bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'idx_food_dict_words', 0, 'c', 0)]\n",
      "[(3, 0, 0, 'SEARCH TABLE food_dict_table USING INDEX idx_food_dict_words (words=?)')]\n"
     ]
    }
   ],
   "source": [
    "# Check if index is created should print out index list idx_food_dict_words will be there\n",
    "cursor.execute(\"PRAGMA index_list(food_dict_table);\")\n",
    "print(cursor.fetchall())\n",
    "# Check if sql is using index you should see using index idx_food_dict_words \n",
    "cursor.execute(f\"\"\"EXPLAIN QUERY PLAN\n",
    "SELECT fdc_id\n",
    "FROM food_dict_table\n",
    "WHERE words = 'pillsbury';\n",
    "\"\"\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a16e0",
   "metadata": {},
   "source": [
    "Step #7 Move over to the Search_Engine.py file to use this data to create our search engine!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
